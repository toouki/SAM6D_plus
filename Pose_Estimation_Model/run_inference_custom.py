# 导入gorilla库（用于配置管理、模型加载等）
import gorilla
# 导入argparse库（用于解析命令行参数）
import argparse
# 导入os库（用于文件路径操作）
import os
# 导入sys库（用于系统相关操作，如路径添加）
import sys
# 从PIL库导入Image（用于图像处理）
from PIL import Image
# 导入os.path并简写为osp（用于路径处理）
import os.path as osp
# 导入numpy并简写为np（用于数值计算）
import numpy as np
# 导入random库（用于随机数生成）
import random
# 导入importlib库（用于动态导入模块）
import importlib
# 导入json库（用于JSON文件读写）
import json

# 导入torch库（PyTorch深度学习框架）
import torch
# 导入torchvision.transforms（用于图像预处理）
import torchvision.transforms as transforms
# 导入cv2库（用于OpenCV图像处理）
import cv2
# 导入math库（用于数学计算）
import math

# 获取当前文件的绝对路径的目录作为BASE_DIR
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# 定义根目录ROOT_DIR（当前目录就是Pose_Estimation_Model）
ROOT_DIR = BASE_DIR
# 将相关模块路径添加到系统路径，以便导入自定义模块
sys.path.append(os.path.join(ROOT_DIR, 'provider'))  # 数据提供相关模块
sys.path.append(os.path.join(ROOT_DIR, 'utils'))     # 工具函数模块
sys.path.append(os.path.join(ROOT_DIR, 'model'))     # 模型定义模块
sys.path.append(os.path.join(ROOT_DIR, 'model', 'pointnet2'))  # PointNet++相关模块


def get_parser():
    # 创建命令行参数解析器，描述为"Pose Estimation"
    parser = argparse.ArgumentParser(description="Pose Estimation")
    # pem（Pose Estimation Model）相关参数
    parser.add_argument("--gpus",  # 指定使用的GPU
                        type=str,
                        default="0",
                        help="GPU indices (e.g., 0 or 0,1)")
    parser.add_argument("--model",  # 指定模型名称
                        type=str,
                        default="pose_estimation_model",
                        help="Model module name")
    parser.add_argument("--config",  # 指定配置文件路径
                        type=str,
                        default="Pose_Estimation_Model/config/base.yaml",
                        help="Path to config file")
    parser.add_argument("--iter",  # 测试时使用的迭代次数（模型 checkpoint）
                        type=int,
                        default=600000,
                        help="Iteration number for testing checkpoint")
    parser.add_argument("--exp_id",  # 实验ID，用于区分不同实验
                        type=int,
                        default=0,
                        help="Experiment ID")
    
    # 输入数据相关参数
    parser.add_argument("--output_dir",  # 输出目录根路径
                        nargs="?", help="Path to root directory of the output")
    parser.add_argument("--cad_path",  # CAD模型路径（单位：mm）
                        nargs="?", help="Path to CAD model (mm)")
    parser.add_argument("--rgb_path",  # RGB图像路径
                        nargs="?", help="Path to RGB image")
    parser.add_argument("--depth_path",  # 深度图像路径（单位：mm）
                        nargs="?", help="Path to Depth image (mm)")
    parser.add_argument("--cam_path",  # 相机参数信息路径
                        nargs="?", help="Path to camera information")
    parser.add_argument("--seg_path",  # 分割信息路径（由ISM生成）
                        nargs="?", help="Path to segmentation information (generated by ISM)")
    parser.add_argument("--det_score_thresh",  # 检测分数阈值（过滤低置信度检测结果）
                        type=float, default=0.2, help="Detection score threshold")
    # 姿态去重参数
    parser.add_argument("--rot_thresh",  # 旋转误差阈值（度）
                        type=float, default=3.0, help="Rotation error threshold (degrees)")
    parser.add_argument("--trans_thresh",  # 平移误差阈值（mm）
                        type=float, default=5.0, help="Translation error threshold (mm)")
    # 解析命令行参数并返回
    args_cfg = parser.parse_args()
    return args_cfg

def init():
    # 解析命令行参数
    args = get_parser()
    # 构建实验名称：模型名_配置文件名_ID+实验ID
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    # 构建日志目录路径
    log_dir = osp.join("log", exp_name)

    # 从配置文件加载配置
    cfg = gorilla.Config.fromfile(args.config)
    # 将命令行参数更新到配置中
    cfg.exp_name = exp_name          # 实验名称
    cfg.gpus     = args.gpus         # GPU索引
    cfg.model_name = args.model      # 模型名称
    cfg.log_dir  = log_dir           # 日志目录
    cfg.test_iter = args.iter        # 测试迭代次数

    # 输入数据路径配置
    cfg.output_dir = args.output_dir  # 输出目录
    cfg.cad_path = args.cad_path      # CAD模型路径
    cfg.rgb_path = args.rgb_path      # RGB图像路径
    cfg.depth_path = args.depth_path  # 深度图像路径
    cfg.cam_path = args.cam_path      # 相机参数路径
    cfg.seg_path = args.seg_path      # 分割结果路径

    # 检测分数阈值和姿态去重阈值配置
    cfg.det_score_thresh = args.det_score_thresh
    cfg.rot_thresh = args.rot_thresh
    cfg.trans_thresh = args.trans_thresh

    # 设置可见的GPU设备
    gorilla.utils.set_cuda_visible_devices(gpu_ids = cfg.gpus)

    return  cfg  # 返回配置对象


# 从data_utils导入数据处理工具函数
from utils.data_utils import (
    load_im,  # 加载图像
    get_bbox,  # 获取边界框
    get_point_cloud_from_depth,  # 从深度图生成点云
    get_resize_rgb_choose,  # 获取Resize后的RGB图像的选择点
)
# 从draw_utils导入绘制检测结果的函数
from utils.draw_utils import draw_detections
# 导入pycocotools.mask用于处理COCO格式的掩码
import pycocotools.mask as cocomask
# 导入trimesh用于处理3D模型
import trimesh

# 定义RGB图像的预处理转换：转为Tensor并Tensor并标准化（使用ImageNet的均值和标准差）
rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize_single(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """可视化单个实例的检测结果"""
    # 将单个实例的姿态包装成列表以适应draw_detections函数
    img = draw_detections(rgb, [pred_rot], [pred_trans], model_points, [K], color=(255, 0, 0))
    # 将绘制结果转为PILPIL图像并保存
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    # 读取保存的预测结果图像
    prediction = Image.open(save_path)
    
    # 并排拼接原图和预测结果
    rgb = Image.fromarray(np.uint8(rgb))  # 原图转为PIL图像
    img = np.array(img)  # 预测结果转为numpy数组
    # 创建拼接图像（宽度为两者之和，高度为原图高度）
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))  # 粘贴原图到左侧
    concat.paste(prediction, (img.shape[1], 0))  # 粘贴预测结果到右侧
    return concat


def _get_template(path, cfg, tem_index=1):
    # 构建模板的RGB、掩码、点云路径
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    # 加载模板RGB图像并转为uint8格式
    rgb = load_im(rgb_path).astype(np.uint8)
    # 加载模板点云并转为float32，单位从mm转为m（除以1000）
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    # 加载模板掩码并转为布尔值（255为前景）
    mask = load_im(mask_path).astype(np.uint8) == 255

    # 获取掩码的边界框（y1:上边界, y2:下边界, x1:左边界, x2:右边界）
    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    # 根据边界框裁剪掩码
    mask = mask[y1:y2, x1:x2]

    # 将RGB图像从BGR转为RGB（因为OpenCV默认加载为BGR），并根据边界框裁剪
    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    # 如果配置了RGB掩码，用掩码过滤RGB图像（只保留前景）
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    # 将裁剪后的RGB图像resize到配置的图像尺寸，使用线性插值
    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    # 应用预处理转换（转Tensor并标准化）
    rgb = rgb_transform(np.array(rgb))

    # 获取掩码中非零区域的索引（展平后）
    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    # 采样模板点（确保采样数量为配置的n_sample_template_point）
    if len(choose) <= cfg.n_sample_template_point:
        # 若点数不足，允许重复采样
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        # 若点数充足，无重复采样
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]  # 筛选采样后的索引
    # 根据边界框裁剪点云，展平后按采样索引选择点
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    # 获取Resize后RGB图像中对应采样点的索引
    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz  # 返回处理后的模板RGB、采样索引、点云


def get_templates(path, cfg):
    # 获取配置的模板视图数量
    n_template_view = cfg.n_template_view
    # 存储所有模板的RGB、采样索引、点云
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    # 总模板视图数量为42
    total_nView = 42
    # 遍历每个模板视图
    for v in range(n_template_view):
        # 计算当前视图索引（均匀从42个视图中选择n_template_view个）
        i = int(total_nView / n_template_view * v)
        # 获取单个模板的信息
        tem, tem_choose, tem_pts = _get_template(path, cfg, i)
        # 将模板数据转为Tensor并移动到GPU，添加到列表（增加批次维度）
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).cuda())
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).cuda())
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).cuda())
    return all_tem, all_tem_pts, all_tem_choose  # 返回所有模板的信息


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg):
    # 存储过滤后的检测结果
    dets = []
    # 加载分割结果JSON文件
    with open(seg_path) as f:
        dets_ = json.load(f)  # 键包括：scene_id, image_id, category_id, bbox, score, segmentation
    # 过滤掉分数低于阈值的检测结果
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_  # 删除原始检测结果，释放内存

    # 加载相机参数
    cam_info = json.load(open(cam_path))
    # 解析相机内参矩阵K（3x3）
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    # 加载RGB图像并转为uint8格式
    whole_image = load_im(rgb_path).astype(np.uint8)
    # 若图像为灰度图，转为RGB格式（复制3通道）
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    # 加载深度图像，转为float32，乘以深度缩放因子并转为米（除以1000）
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    # 从深度图生成点云（3D坐标）
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    # 加载CAD模型
    mesh = trimesh.load_mesh(cad_path)
    # 从CAD模型采样点（数量为配置的n_sample_model_point），单位转为米（除以1000）
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    # 计算模型点云中的最大半径（用于过滤离群点）
    radius = np.max(np.linalg.norm(model_points, axis=1))


    # 存储处理后的测试数据
    all_rgb = []          # 处理后的RGB图像
    all_cloud = []        # 处理后的点云
    all_rgb_choose = []   # RGB图像的采样索引
    all_score = []        # 检测分数
    all_dets = []         # 检测结果

    # 遍历每个检测实例
    for inst in dets:
        seg = inst['segmentation']  # 分割掩码（COCO RLE格式）
        score = inst['score']       # 检测分数

        # 处理掩码
        h,w = seg['size']  # 掩码尺寸（高度、宽度）
        try:
            # 将RLE格式转为掩码对象
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg  # 若转换失败，直接使用原始seg
        # 解码RLE得到掩码（二进制矩阵）
        mask = cocomask.decode(rle)
        # 过滤掩码：只保留深度图有效区域（深度>0）
        mask = np.logical_and(mask > 0, whole_depth > 0)
        # 若掩码有效像素数>32，计算边界框；否则跳过该实例
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        # 根据边界框裁剪掩码
        mask = mask[y1:y2, x1:x2]
        # 获取掩码中非零区域的索引（展平后）
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # 处理点云
        # 根据边界框裁剪点云，展平后按掩码索引选择点
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        # 计算点云中心
        center = np.mean(cloud, axis=0)
        # 点云去中心化（减去中心）
        tmp_cloud = cloud - center[None, :]
        # 过滤离群点：保留在模型半径1.2倍范围内的点
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        # 若有效点<4，跳过该实例
        if np.sum(flag) < 4:
            continue
        # 根据过滤结果更新索引和点云
        choose = choose[flag]
        cloud = cloud[flag]

        # 采样观测点（确保数量为配置的n_sample_observed_point）
        if len(choose) <= cfg.n_sample_observed_point:
            # 点数不足，允许重复采样
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            # 点数充足，无重复采样
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]  # 更新索引
        cloud = cloud[choose_idx]    # 更新点云

        # 处理RGB图像
        # 根据边界框裁剪RGB图像，并从BGR转为RGB
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        # 若配置了RGB掩码，用掩码过滤RGB图像
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        # resize到配置的图像尺寸，线性插值
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        # 应用预处理转换
        rgb = rgb_transform(np.array(rgb))
        # 获取Resize后RGB图像中对应采样点的索引
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        # 将处理后的数据添加到列表
        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        all_dets.append(inst)

    # 构建返回字典，将数据转为Tensor并移动到GPU
    ret_dict = {}
    ret_dict['pts'] = torch.stack(all_cloud).cuda()  # 点云（批次维度）
    ret_dict['rgb'] = torch.stack(all_rgb).cuda()    # RGB图像（批次维度）
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).cuda()  # 采样索引（批次维度）
    ret_dict['score'] = torch.FloatTensor(all_score).cuda()  # 检测分数（批次维度）

    # 获取实例数量
    ninstance = ret_dict['pts'].size(0) if len(all_cloud) > 0 else 0
    # 复制模型点云，适配批次维度（每个实例共享相同模型点云）
    if ninstance > 0:
        ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
        # 复制相机内参，适配批次维度
        ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    # 返回处理后的数据、原图、完整点云、模型点云、检测结果
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets


# 姿态去重相关函数
def compute_rotation_error(R1, R2):
    """计算两个旋转矩阵的误差（度）"""
    # R1: [3,3], R2: [3,3]
    R_diff = R1.T @ R2  # 旋转差
    trace = np.trace(R_diff)
    trace = np.clip(trace, -1.0 + 1e-6, 3.0 - 1e-6)  # 数值稳定性处理
    angle_rad = np.arccos((trace - 1) / 2)
    return np.rad2deg(angle_rad)

def compute_translation_error(t1, t2):
    """计算两个平移向量的误差（mm）"""
    # t1: [3], t2: [3]
    return np.linalg.norm(t1 - t2)

def pose_nms(poses_rot, poses_trans, scores, rot_thresh, trans_thresh):
    """
    姿态非极大值抑制，去除重复姿态
    输入:
        poses_rot: [N, 3, 3] 旋转矩阵列表
        poses_trans: [N, 3] 平移向量列表（mm）
        scores: [N] 姿态分数
        rot_thresh: 旋转误差阈值（度）
        trans_thresh: 平移误差阈值（mm）
    输出:
        keep_indices: 保留的姿态索引
    """
    if len(poses_rot) == 0:
        return []
    
    # 按分数降序排序索引
    sorted_indices = np.argsort(scores)[::-1]
    keep = []
    
    for i in sorted_indices:
        current_rot = poses_rot[i]
        current_trans = poses_trans[i]
        is_duplicate = False
        
        # 与已保留的姿态比较
        for j in keep:
            rot_err = compute_rotation_error(current_rot, poses_rot[j])
            trans_err = compute_translation_error(current_trans, poses_trans[j])
            if rot_err < rot_thresh and trans_err < trans_thresh:
                is_duplicate = True
                break
        
        if not is_duplicate:
            keep.append(i)
    
    return keep


if __name__ == "__main__":
    # 初始化配置
    cfg = init()

    # 设置随机种子，保证实验可复现
    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # 加载模型
    print("=> creating model ...")
    # 动态导入模型模块
    MODEL = importlib.import_module(cfg.model_name)
    # 创建模型实例
    model = MODEL.Net(cfg.model)
    # 将模型移动到GPU
    model = model.cuda()
    # 设置模型为评估模式
    model.eval()
    # 定义预训练模型 checkpoint 路径
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    # 加载模型权重
    gorilla.solver.load_checkpoint(model=model, filename=checkpoint)

    # 提取模板特征
    print("=> extracting templates ...")
    # 模板路径
    tem_path = os.path.join(cfg.output_dir, 'templates')
    # 获取所有模板的信息
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset)
    # 在无梯度计算模式下提取模板特征
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    # 加载测试数据
    print("=> loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset
    )
    # 获取实例数量
    ninstance = input_data['pts'].size(0) if 'pts' in input_data else 0
    
    if ninstance == 0:
        print("=> No valid instances found!")
        sys.exit(0)
    
    # 运行模型推理
    print("=> running model ...")
    with torch.no_grad():  # 无梯度计算
        # 将模板特征适配批次维度（每个实例共享模板特征）
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
        # 模型前向传播，得到输出
        out = model(input_data)

    # 计算姿态分数（检测分数×姿态预测分数）
    if 'pred_pose_score' in out.keys():
        pose_scores = out['pred_pose_score'] * out['score']
    else:
        pose_scores = out['score']
    # 将分数从GPU移到CPU并转为numpy数组
    pose_scores = pose_scores.detach().cpu().numpy()
    # 提取预测的旋转矩阵（从GPU移到CPU，转为numpy）
    pred_rot = out['pred_R'].detach().cpu().numpy()  # [N, 3, 3]
    # 提取预测的平移向量（从GPU移到CPU，转为numpy，单位从米转为毫米）
    pred_trans = out['pred_t'].detach().cpu().numpy() * 1000  # [N, 3]

    # 新增：应用姿态NMS去除重复姿态
    print("=> applying pose NMS ...")
    keep_indices = pose_nms(
        poses_rot=pred_rot,
        poses_trans=pred_trans,
        scores=pose_scores,
        rot_thresh=cfg.rot_thresh,
        trans_thresh=cfg.trans_thresh
    )
    print(f"=> NMS retained {len(keep_indices)}/{ninstance} instances")

    keep_indices = keep_indices[:3]

    # 根据NMS结果筛选
    pred_rot = pred_rot[keep_indices]
    pred_trans = pred_trans[keep_indices]
    pose_scores = pose_scores[keep_indices]
    detections = [detections[i] for i in keep_indices]

    # 保存结果
    print("=> saving results ...")
    # 定义输出目录
    output_dir = f"{cfg.output_dir}/sam6d_results"
    # 创建输出目录（若不存在）
    os.makedirs(output_dir, exist_ok=True)
    
    # 为每个实例保存结果
    for idx, det in enumerate(detections):
        # 更新检测结果中的分数和姿态
        detections[idx]['score'] = float(pose_scores[idx])  # 姿态分数
        detections[idx]['R'] = pred_rot[idx].tolist()  # 旋转矩阵
        detections[idx]['t'] = pred_trans[idx].tolist()  # 平移向量（毫米）
        
        # 保存单个实例的可视化结果
        K = input_data['K'].detach().cpu().numpy()[keep_indices[idx]]  # 当前实例的相机内参
        # K = input_data['K'].detach().cpu().numpy()[idx]  # 当前实例的相机内参
        save_path = os.path.join(output_dir, f'vis_pem_instance_{idx}.png')  # 可视化保存路径
        # 可视化并保存（model_points单位转为mm）
        vis_img = visualize_single(img, pred_rot[idx], pred_trans[idx], model_points*1000, K, save_path)
        vis_img.save(save_path)

    # 保存所有实例的检测结果到JSON文件
    with open(os.path.join(output_dir, 'detection_pem.json'), "w") as f:
        json.dump(detections, f)

    # 打印完成信息
    print(f"=> 已保存所有{len(detections)}个实例的检测结果到 {output_dir}")